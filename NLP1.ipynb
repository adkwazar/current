{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Podsumowanie i uzupłenienie wiadomości o Napisach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"To jest przykladowy tekst.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0] #zerowy znak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o j'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1:4] #znaki: 1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ex\" in x #czy fragment \"ex\" jest w napisie x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"and\" not in x #czy prawdą jest, ze \"and\" nie znajduje sie w napisie x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[-1] #ostatni znak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'st przykladowy tekst.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[5:] #znaki od 5 do konca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ekst.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[-5:] #ostatnie 5 znakow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To', 'jest', 'przykladowy', 'tekst.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.split(\" \") #podziel tekst ze wzgledu na spacje i zapisz poszczegolne slowa do listy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T', ' jest przyklad', 'wy tekst.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.split(\"o\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.startswith(\"Th\") #czy tekst zapisany w zmiennej x zaczyna sie od Th?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.endswith(\".\") #czy tekst zapisany w zmiennej x konczy sie kropką?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.count(\"e\") #policz liczbe wystapien litery \"e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To jXst przykladowy tXkst.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.replace(\"e\",\"X\") #zamien wszystkie wystapienia \"e\" na \"X\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TO JEST PRZYKLADOWY TEKST.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.upper()  #zamien wszystkie znaki na duze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to jest przykladowy tekst.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.lower()  #zamien wszystkie znaki na male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tO JEST PRZYKLADOWY TEKST.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.swapcase() #te co byly duze niech beda male, a male duze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join([\"py\",\"t\",\"hon\"]) #zlacz liste slow w jedno slowo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'py_t_hon'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"_\".join([\"py\",\"t\",\"hon\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation #znaki interpunkcyjne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.digits #cyfry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> NLTK - biblioteka do przetwarzania języka naturalnego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Tokenizacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_text = \"There are different types of dementia, a term for a loss of cognitive function, including Alzheimer's disease and Lewy body dementia. Sometimes more than one type can happen at the same time, while in other cases, dementia is a side effect of another condition, like a head injury, excess drinking, or blood clots, for example. It may also be caused by certain medications.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"There are different types of dementia, a term for a loss of cognitive function, including Alzheimer's disease and Lewy body dementia.\", 'Sometimes more than one type can happen at the same time, while in other cases, dementia is a side effect of another condition, like a head injury, excess drinking, or blood clots, for example.', 'It may also be caused by certain medications.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(ex_text)) #podzial na sentencje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie1: Porównaj zastosowanie metody split z kropką oraz metody sent_tokenize na tekście: \"Mr. Smith is a scientist. He is also a very good teacher.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'different', 'types', 'of', 'dementia', ',', 'a', 'term', 'for', 'a', 'loss', 'of', 'cognitive', 'function', ',', 'including', 'Alzheimer', \"'s\", 'disease', 'and', 'Lewy', 'body', 'dementia', '.', 'Sometimes', 'more', 'than', 'one', 'type', 'can', 'happen', 'at', 'the', 'same', 'time', ',', 'while', 'in', 'other', 'cases', ',', 'dementia', 'is', 'a', 'side', 'effect', 'of', 'another', 'condition', ',', 'like', 'a', 'head', 'injury', ',', 'excess', 'drinking', ',', 'or', 'blood', 'clots', ',', 'for', 'example', '.', 'It', 'may', 'also', 'be', 'caused', 'by', 'certain', 'medications', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(ex_text)) #podzial na slowa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie2: Zdefiniuj funkcję, która dla danego tekstu zwraca liste słów z małej litery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie3: Załaduj plik tekstowy o nazwie \"example_text.txt\", a następnie zapisz pod zmienną  $txt\\_words$  liste słów (z małej litery) występujących w tekście. Dodatkowo: <br>\n",
    "    \n",
    "- Wyświetl pierwsze 20 słów.\n",
    "- Ile słów znajduje się w tekście?\n",
    "- Ile występuje słów unikatowych?\n",
    "- Wyznacz tzw. miarę Herdana  $C=\\frac{\\log V}{\\log M}$ , gdzie  V  - liczba różnych słów,  M  - liczba wszystkich słów.\n",
    "- Ile razy wystąpiło słowo $woman$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pobierz korpus https://data.nls.uk/data/digitised-collections/a-medical-history-of-british-india/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No', '.', '1111', '(', 'Sanitary', '),', 'dated', 'Ootacamund', ',', 'the']\n"
     ]
    }
   ],
   "source": [
    "#Tokenizacja korpusu\n",
    "\n",
    "from nltk.corpus import PlaintextCorpusReader #do czytania kolekcji dokumentow\n",
    "\n",
    "\n",
    "corpus_root = 'nls-text-indiaPapers/' #nazwa katalogu w ktorym znajdują sie dokumenty tekstowe\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*', encoding='latin1') #pod zmienna wordlists zapisz wszystkie pliki wsytepujace w sciezce podanej powyzej\n",
    "corpus_tokens = wordlists.words()  #metoda do tokenizacji tego zbioru dokumentow i zapisaniu w postaci listy tokenów (słów)\n",
    "\n",
    "print(corpus_tokens[:10]) #wyswietl pierwsze 10 slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stopwords Corpus  This corpus contains lists of stop words for several languages.  These are high-frequency grammatical words which are usually ignored in text retrieval applications.  They were obtained from: http://anoncvs.postgresql.org/cvsweb.cgi/pgsql/src/backend/snowball/stopwords/  The stop words for the Romanian language were obtained from: http://arlc.ro/resources/  The English list has been augmented https://github.com/nltk/nltk_data/issues/22  The German list has been corrected https://github.com/nltk/nltk_data/pull/49  A Kazakh list has been added https://github.com/nltk/nltk_data/pull/52  A Nepali list has been added https://github.com/nltk/nltk_data/pull/83  An Azerbaijani list has been added https://github.com/nltk/nltk_data/pull/100  A Greek list has been added https://github.com/nltk/nltk_data/pull/103  An Indonesian list has been added https://github.com/nltk/nltk_data/pull/112 '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.readme().replace('\\n', ' ') #opis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'such', 'on', 'some', 'down', 'but', 'isn', 'to', \"mustn't\", \"haven't\", 'or', 'any', 'did', 'most', \"should've\", 'o', 'few', \"wasn't\", 'yours', 'them', \"she's\", 'up', 'y', 'too', 'after', 'its', 'of', 'wouldn', 'no', 'yourselves', 't', 'should', 'ours', \"you've\", \"you'd\", \"you're\", 'where', 'll', 'him', \"needn't\", 'once', 'had', 'that', 'aren', 'they', 'he', 'this', 'their', 'couldn', 'needn', \"you'll\", 'was', 'mightn', 'do', 'are', 'who', 'didn', 'what', 'having', \"shan't\", 've', \"weren't\", 'into', 'me', 'be', 'wasn', 'hasn', 'there', 'a', 'only', 'yourself', 'below', 'so', 'ain', 'were', 'these', 'those', \"couldn't\", 'itself', 'the', \"hadn't\", 'it', \"hasn't\", 'has', 'over', 'herself', 'her', 'until', 'we', 'won', 'all', 'nor', 'each', 'you', 'ma', 'both', 'other', 'doesn', 'hers', 'if', \"mightn't\", 'am', 'same', 'with', 'from', 'theirs', \"aren't\", 'more', \"it's\", 'when', 'is', \"doesn't\", 'hadn', 'shan', 'ourselves', 'between', 'not', 'an', 'whom', 'off', 'here', 'as', \"shouldn't\", 'shouldn', 'your', 'she', 'under', 'during', 'why', 'which', \"that'll\", 'own', 'and', \"won't\", 'in', \"isn't\", 'myself', 'for', 'have', 'will', \"didn't\", \"don't\", 'weren', 'being', 'just', 'themselves', 'further', 'his', 'before', 'does', 'can', 'at', 'through', 'been', \"wouldn't\", 'doing', 'm', 'again', 's', 'because', 'than', 're', 'now', 'mustn', 'by', 'out', 'our', 'i', 'against', 'above', 'then', 'd', 'how', 'don', 'while', 'haven', 'himself', 'about', 'my', 'very'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\")) #angielskie stopwords\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'bengali', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.fileids()) #inne języki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_sentence = \"This is an example 1, showing off stop words filtration that occur using NLTK library.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', '1', ',', 'showing', 'off', 'stop', 'words', 'filtration', 'that', 'occur', 'using', 'NLTK', 'library', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(ex_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', '1', ',', 'showing', 'stop', 'words', 'filtration', 'occur', 'using', 'NLTK', 'library', '.']\n"
     ]
    }
   ],
   "source": [
    "print([elem for elem in word_tokenize(ex_sentence) if elem not in stop_words ]) #bez stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie4: Zapisz do listy  $pure\\_words$  słowa występujące w napisie  $ex\\_sentence$  niebędące stop words, znakami interpunkcyjnymi czy cyframi (oczywiście w sposób automatyczny)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie5: Utwórz słownik zawierający jako klucze  stopwords  a wartościami niech będzie liczba ich występień (ze wszystkich dokumentów zawartych w 'nls-text-indiaPapers/'). Które z nich występowało najczęściej?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Klasyfikacja języka na podstawie stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_text = \"The mitochondrial dysfunction that occurs from a tumor's microenvironment is called terminal exhaustion. Understanding this mitochondrial state and the condition that provokes it could lead to new targets in drug development.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize #tokenizacja ze względu na interpunkcje i spacje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'mitochondrial', 'dysfunction', 'that', 'occurs', 'from', 'a', 'tumor', \"'\", 's', 'microenvironment', 'is', 'called', 'terminal', 'exhaustion', '.', 'Understanding', 'this', 'mitochondrial', 'state', 'and', 'the', 'condition', 'that', 'provokes', 'it', 'could', 'lead', 'to', 'new', 'targets', 'in', 'drug', 'development', '.']\n"
     ]
    }
   ],
   "source": [
    "ex_tokens = wordpunct_tokenize(ex_text)\n",
    "\n",
    "print(ex_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arabic': 0, 'azerbaijani': 2, 'bengali': 0, 'danish': 0, 'dutch': 2, 'english': 11, 'finnish': 0, 'french': 1, 'german': 1, 'greek': 0, 'hungarian': 2, 'indonesian': 0, 'italian': 2, 'kazakh': 0, 'nepali': 0, 'norwegian': 0, 'portuguese': 1, 'romanian': 2, 'russian': 0, 'slovene': 4, 'spanish': 1, 'swedish': 0, 'tajik': 0, 'turkish': 0}\n"
     ]
    }
   ],
   "source": [
    "language_ratios = {} #tworze pusty slownik\n",
    "\n",
    "ex_words = [word.lower() for word in ex_tokens] #zamieniam wszystkie litery na male \n",
    "unique_words_set = set(ex_words) #slowa unikatowe zapisuje w postaci zbioru\n",
    "\n",
    "for language in stopwords.fileids(): #dla kazdego jezyka \n",
    "    stopwords_set = set(stopwords.words(language)) #do stopwords_set zapisuje stopwords (danego jezyka) w formie zbioru\n",
    "    common_elements = unique_words_set.intersection(stopwords_set) #przeciecie zbiorow (A.intersection(B) to inaczej A∩B)\n",
    "    language_ratios[language] = len(common_elements) #zaliczam ile wspolnych elementow ma dany jezyk a wystepujace w tekscie stop words\n",
    "    \n",
    "\n",
    "print(language_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english\n"
     ]
    }
   ],
   "source": [
    "most_rated_language = max(language_ratios, key=language_ratios.get) #zapisuje jezyk ktory ma najwiecej stop_words w tekscie\n",
    "print(most_rated_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'that', 'this', 'in', 'is', 'and', 'the', 'from', 's', 'to', 'it', 'a'}\n"
     ]
    }
   ],
   "source": [
    "print(unique_words_set.intersection(set(stopwords.words(most_rated_language)))) #jakie są to słowa? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'drug', 's', 'in', 'to'}\n"
     ]
    }
   ],
   "source": [
    "#ciekawe są stop_words słoweńskie wystepujace w tekscie \n",
    "\n",
    "print(unique_words_set.intersection(set(stopwords.words('slovene')))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Zastosowanie wyrażeń regularnych w NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [abc] <-- a lub b lub c\n",
    "- [A-Z] <-- od A do Z\n",
    "- [^X] <-- wszystko z wyjątkiem X\n",
    "- . <-- cokolwiek\n",
    "- \\d <-- dowolna cyfra od 0 do 9\n",
    "- \\D <-- wszystko z wyjątkiem cyfr [^d]\n",
    "- \\s <-- spacja\n",
    "- \\S <-- wszystko co nie jest spacją\n",
    "- \\w <-- a-z, A-Z, cyfry, podkreślenie _\n",
    "- *<-- żadne lub dowolnej długości powtórzenie, np ca*t znaczy ct, cat, caat, caaat...\n",
    "- +<-- jedno lub więcej powtórzenie, np ca+t znaczy cat, caat, caaat...\n",
    "- ? <-- żadne wystąpienie lub jedno wystąpienie, np pyt?hon znaczy pyhon lub python\n",
    "- {n} <-- znaczy, że coś ma nastąpić n razy\n",
    "- {n,m} <-- znaczy, że coś ma nastąpić między n a m razy np ab{1,3}c znaczy abc, abbc, abbbc\n",
    "- (X|Y) <-- X lub Y\n",
    "- ^x <-- znaczy, że od x ma sie zacząć wyraz\n",
    "- x$ <-- znaczy, że na x ma sie kończyc wyraz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie6: Podaj po dwa przykłady wyrazów (mające sens lub nie), które spełniają następujące wyrażenia regularne:\n",
    "    \n",
    "    \n",
    "    \n",
    "- '.ma.*'\n",
    "- 'meal?'\n",
    "- 'go{2,6}gle'\n",
    "- 'a[knm]e'\n",
    "- 'b[^a]d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #biblioteka do wyrażeń regularnych w Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['women', 'women', 'women', 'woman', 'woman', 'woman', 'woman', 'woman', 'woman', 'woman', 'woman', 'woman', 'women', 'woman', 'women', 'women', 'woman', 'women', 'woman', 'women', 'woman', 'woman', 'woman', 'woman', 'women', 'women', 'women', 'women', 'women', 'woman', 'woman', 'women', 'women', 'women', 'women', 'women', 'women', 'woman', 'woman', 'women', 'women', 'women']\n"
     ]
    }
   ],
   "source": [
    "#Uwaga: txt_words trzeba sobie utworzyc w zadaniu 3\n",
    "\n",
    "womaen_strings=[w for w in txt_words if re.search('^wom[ae]n$', w)] #znajdź w liście wyrazów txt_words słowa spełniające podane wyrażenie regularne. Co ono oznacza?\n",
    "print(womaen_strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie7: Wykonaj analogiczne przeszukiwanie z wzorcem 'wom[ae]n' i porównaj otrzymane wyniki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie8: Wykonaj analogiczne przeszukiwanie, tym razem poszukując słów o łącznej liczbie znaków 13, które zaczynają się od 'a'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie9: Wykonaj analogiczne przeszukiwanie, tym razem poszukując słów o łącznej liczbie znaków 13, które nie zaczynają się od małej litery.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Własna tokenizacja z użyciem wyrazeń regularnych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"[a-zA-Z]+\") #tak definiuje slowo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"The genome of Bacteroides vulgatus was found to contain DNA that belongs to a virus they called BV01. Next, they had to determine whether the virus could escape or reinfect the host. The researchers found that conditions in the gut may act to stimulate the activity of BV01.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'genome', 'of', 'Bacteroides', 'vulgatus', 'was', 'found', 'to', 'contain', 'DNA', 'that', 'belongs', 'to', 'a', 'virus', 'they', 'called', 'BV', 'Next', 'they', 'had', 'to', 'determine', 'whether', 'the', 'virus', 'could', 'escape', 'or', 'reinfect', 'the', 'host', 'The', 'researchers', 'found', 'that', 'conditions', 'in', 'the', 'gut', 'may', 'act', 'to', 'stimulate', 'the', 'activity', 'of', 'BV']\n"
     ]
    }
   ],
   "source": [
    "s_tokenized = tokenizer.tokenize(s)\n",
    "\n",
    "print(s_tokenized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
