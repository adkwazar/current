{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Części mowy (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The Very Large Telescope (VLT) of the European Southern Observatory (ESO), an array of four individual telescopes in the Atacama desert, has given us a huge amount of new data about the universe. Researchers have now used it to find a group of six galaxies around a supermassive black hole, from when the Universe was just 0.9 billion years old - it's estimated to be 13.8 billion years old now. Black holes are thought to sit at the center of galaxies including the Milky Way. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the very large telescope (vlt) of the european southern observatory (eso), an array of four individual telescopes in the atacama desert, has given us a huge amount of new data about the universe.', \"researchers have now used it to find a group of six galaxies around a supermassive black hole, from when the universe was just 0.9 billion years old - it's estimated to be 13.8 billion years old now.\", 'black holes are thought to sit at the center of galaxies including the milky way.']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text.lower()) #podzial na sentencje \n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'very', 'large', 'telescope', '(', 'vlt', ')', 'of', 'the', 'european', 'southern', 'observatory', '(', 'eso', ')', ',', 'an', 'array', 'of', 'four', 'individual', 'telescopes', 'in', 'the', 'atacama', 'desert', ',', 'has', 'given', 'us', 'a', 'huge', 'amount', 'of', 'new', 'data', 'about', 'the', 'universe', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens_sen1 = word_tokenize(sentences[0]) #podzial na slowa pierwszego zdania\n",
    "print(tokens_sen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 4, 'of': 3, '(': 2, ')': 2, ',': 2, 'very': 1, 'large': 1, 'telescope': 1, 'vlt': 1, 'european': 1, ...})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist(tokens_sen1)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4), ('of', 3)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist.most_common(2) #dwa najczęstsze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('very', 'RB'), ('large', 'JJ'), ('telescope', 'NN'), ('(', '('), ('vlt', 'NN'), (')', ')'), ('of', 'IN'), ('the', 'DT'), ('european', 'JJ'), ('southern', 'JJ'), ('observatory', 'NN'), ('(', '('), ('eso', 'NN'), (')', ')'), (',', ','), ('an', 'DT'), ('array', 'NN'), ('of', 'IN'), ('four', 'CD'), ('individual', 'JJ'), ('telescopes', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('atacama', 'NN'), ('desert', 'NN'), (',', ','), ('has', 'VBZ'), ('given', 'VBN'), ('us', 'PRP'), ('a', 'DT'), ('huge', 'JJ'), ('amount', 'NN'), ('of', 'IN'), ('new', 'JJ'), ('data', 'NNS'), ('about', 'IN'), ('the', 'DT'), ('universe', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag #Części mowy\n",
    "\n",
    "tags = pos_tag(tokens_sen1)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Słowa nieistniejące "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gutvidual', 'JJ')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags2 = pos_tag(['gutvidual']) #a jednak przewiduje... :)\n",
    "tags2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Słowa w zależności od kontekstu mogą mieć różne POS tagi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('can', 'MD'), ('not', 'RB'), ('bear', 'VB'), ('this', 'DT'), ('headache', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag(word_tokenize(\"I cannot bear this headache\"))) #bear jest czasownikiem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Yesterday', 'NN'), (',', ','), ('I', 'PRP'), ('saw', 'VBD'), ('a', 'DT'), ('bear', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag(word_tokenize(\"Yesterday, I saw a bear\")))  #bear jest rzeczownikiem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Uczenie HMM - jaki POS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem:\n",
    "\n",
    "- dana jest sekwencja wyrazów  $x_1x_2x_3...x_n$  (przykładowo [\"I\", \"like\", \"dogs\"])\n",
    "- cel: znaleźć najbardziej prawdopodobną sekwencje tagów  $y_1y_2y_3...y_n$  (przykładowo [\"PRP\",\"VBP\", \"NNS\"]), czyli:\n",
    "\n",
    "$argmax_{y_1,y_2,y_3...,y_n}P(x_1,x_2,x_3,...,x_n,y_1,y_2,y_3,...,y_n)$\n",
    "\n",
    "- Jak? \n",
    "\n",
    "Algorytm Viterbiego\n",
    "\n",
    "- Stany:\n",
    "\n",
    "obserwacje = słowa\n",
    "\n",
    "stany ukryte = tagi POS\n",
    "\n",
    "\n",
    "- Uczenie algorytmu polega na wyznaczeniu:\n",
    "\n",
    "prawdopodobieństw przejść = prawdopodobieństwa typu P(VP|NP)\n",
    "\n",
    "prawdopodobieńsw emisji = prawdopodobieństwa typu P(John|VP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ PENN TREEBANK SAMPLE ]\\r http://www.cis.upenn.edu/~treebank/home.html\\r \\r This is a ~5% fragment of Penn Treebank, (C) LDC 1995.  It is made\\r available under fair use for the purposes of illustrating NLTK tools\\r for tokenizing, tagging, chunking and parsing.  '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import treebank  #korpus do testowania algorytmow przewidujących częsci mowy\n",
    "\n",
    "treebank.readme().replace('\\n', ' ')[:260]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')], [('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "print(treebank.tagged_sents()[:2]) #pierwsze dwie otagowane sentencje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3914"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(treebank.tagged_sents()) #ile wszystkich otagowanych sentencji?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import hmm  #Ukryte modele Markowa\n",
    "\n",
    "trainer = hmm.HiddenMarkovModelTrainer() #buduje model HMMM\n",
    "tagger = trainer.train_supervised(treebank.tagged_sents()) #trenuje model (metoda największej wiarygodności) na wszystkich otagowanych sentencjach z treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('today', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('such', 'PDT'),\n",
       " ('a', 'DT'),\n",
       " ('very', 'RB'),\n",
       " ('nice', 'JJ'),\n",
       " ('day', 'NN')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag(['today','is','such','a','very','nice','day']) #patrze jak model sprawdza sie na przykladowych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('today', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('such', 'PDT'),\n",
       " ('a', 'DT'),\n",
       " ('picturesque', 'NNP'),\n",
       " ('day', 'NNP')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.tag(['today','is','such','a','picturesque','day']) #patrze jak model sprawdza sie na innych przykladowych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('today', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('such', 'JJ'),\n",
       " ('a', 'DT'),\n",
       " ('picturesque', 'JJ'),\n",
       " ('day', 'NN')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(['today','is','such','a','picturesque','day']) #POS tagging przy uzyciu metody pos_tag, dla porownania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9815546902936152"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger.evaluate(treebank.tagged_sents()) #jaka jest przewidywalnosc modelu na całym korpusie treebank?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie1: Podziel wyjściowy zbiór sentencji $treebank.tagged\\_sents()$ na dwa zbiory: pierwsze 3000 sentencje zapisz pod zmienną  $trained$, natomiast pozostałe sentencje zapisz pod zmienną $tested$.Zbuduj model tylko w oparciu o $trained$. Następnie wykonaj ewaluacje modelu zarówno na  $trained$  jak i  $tested$. Zastanów się z czego mogą wynikać rozbieżności."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Porcjowanie (Chunking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('very', 'RB'), ('large', 'JJ'), ('telescope', 'NN'), ('(', '('), ('vlt', 'NN'), (')', ')'), ('of', 'IN'), ('the', 'DT'), ('european', 'JJ'), ('southern', 'JJ'), ('observatory', 'NN'), ('(', '('), ('eso', 'NN'), (')', ')'), (',', ','), ('an', 'DT'), ('array', 'NN'), ('of', 'IN'), ('four', 'CD'), ('individual', 'JJ'), ('telescopes', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('atacama', 'NN'), ('desert', 'NN'), (',', ','), ('has', 'VBZ'), ('given', 'VBN'), ('us', 'PRP'), ('a', 'DT'), ('huge', 'JJ'), ('amount', 'NN'), ('of', 'IN'), ('new', 'JJ'), ('data', 'NNS'), ('about', 'IN'), ('the', 'DT'), ('universe', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import RegexpParser\n",
    "\n",
    "grammar = \"chunk: {<DT>?<JJ>*<NN>}\" #DT - determiner/określnik, JJ - adjective/przymiotnik, NN - noun/rzeczonik\n",
    "chunker = RegexpParser(grammar) \n",
    "result = chunker.parse(tags) #tags zdefiniowane bylo wyzej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  the/DT\n",
      "  very/RB\n",
      "  (chunk large/JJ telescope/NN)\n",
      "  (/(\n",
      "  (chunk vlt/NN)\n",
      "  )/)\n",
      "  of/IN\n",
      "  (chunk the/DT european/JJ southern/JJ observatory/NN)\n",
      "  (/(\n",
      "  (chunk eso/NN)\n",
      "  )/)\n",
      "  ,/,\n",
      "  (chunk an/DT array/NN)\n",
      "  of/IN\n",
      "  four/CD\n",
      "  individual/JJ\n",
      "  telescopes/NNS\n",
      "  in/IN\n",
      "  (chunk the/DT atacama/NN)\n",
      "  (chunk desert/NN)\n",
      "  ,/,\n",
      "  has/VBZ\n",
      "  given/VBN\n",
      "  us/PRP\n",
      "  (chunk a/DT huge/JJ amount/NN)\n",
      "  of/IN\n",
      "  new/JJ\n",
      "  data/NNS\n",
      "  about/IN\n",
      "  (chunk the/DT universe/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 'DT')\n"
     ]
    }
   ],
   "source": [
    "print(result[0]) #mozna sie odwolywac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie2: Zapisz do listy wszystkie chunki występujące w tekście a spełniajacy zadany (przez siebie) warunek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.draw() #rysowanie tego co wyzej (pojawia sie nowe okno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie3: Zinterpretuj poniższy chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"chunk: {<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\" #co oznacza wyrazenie regularne zapisane w {}?\n",
    "chunker = RegexpParser(grammar) \n",
    "result = chunker.parse(tags) #tags zdefiniowane bylo wyzej\n",
    "result.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie4: Pobierz dowolny tekst (przez f = open(...)). Następnie wyznacz dla niego zaproponowany przez siebie chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Rozpoznawanie bytów (Entity Recognition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Albert', 'NNP'), ('Einstein', 'NNP'), ('was', 'VBD'), ('born', 'VBN'), ('at', 'IN'), ('Ulm', 'NNP'), (',', ','), ('in', 'IN'), ('Württemberg', 'NNP'), (',', ','), ('Germany', 'NNP'), (',', ','), ('on', 'IN'), ('March', 'NNP'), ('14', 'CD'), (',', ','), ('1879', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tokens_sen2 = word_tokenize(\"Albert Einstein was born at Ulm, in Württemberg, Germany, on March 14, 1879.\")\n",
    "tags2 = pos_tag(tokens_sen2)\n",
    "print(tags2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NE Albert/NNP Einstein/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  at/IN\n",
      "  (NE Ulm/NNP)\n",
      "  ,/,\n",
      "  in/IN\n",
      "  (NE Württemberg/NNP)\n",
      "  ,/,\n",
      "  (NE Germany/NNP)\n",
      "  ,/,\n",
      "  on/IN\n",
      "  March/NNP\n",
      "  14/CD\n",
      "  ,/,\n",
      "  1879/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "\n",
    "namedEnt = ne_chunk(tags2, binary = True) #przeszukiwanie pod kątem osób, miejsc itd...\n",
    "print(namedEnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "namedEnt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie5: Znajdz wszystkie byty (entity) w tekście poniżej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Google, LLC is an American multinational technology company that specializes in Internet-related services and products, which include online advertising technologies, a search engine, cloud computing, software, and hardware. It is considered one of the Big Five technology companies in the U.S. information technology industry, alongside Amazon, Facebook, Apple, and Microsoft.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Stemowanie (Stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n",
      "wait\n",
      "wait\n",
      "wait\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer() #Tworze stemer\n",
    "\n",
    "words = [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
    "\n",
    "for elem in words:\n",
    "    print(ps.stem(elem)) #stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie6: Napisz funkcję do Stemmingu zdań. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wskazówka: Podziel tekst na tokeny, potem dokonaj stemmingu na każdym tokenie, zapisz wyrazy do listy, na końcu z użyciem join połącz wszystko w jeden ciągły tekst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Lematyzacja (Lemmatization) - \"lepszy Stemming\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer() #tworze Lemmatyzator \n",
    "print(lemmatizer.lemmatize(\"cats\")) #wywołuje metode lemmatize z tego Lemmatyzatora na słowie 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foot\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"feet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"better\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"better\", pos = 'r')) #n-noun/rzeczownik (domyslnie), a-adjective/przymiotnik, v-verb/czasownik, r-adverb/przyslowek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie7: Napisz funkcję do Lemmatyzacji zdań. W tym przypadku użyteczne mogą być tagi POS:\n",
    "        \n",
    "- zaczynające się od J uznaj za przymiotniki\n",
    "- zaczynające się od N to rzeczowniki\n",
    "- zaczynające się od V do czasowniki\n",
    "- zaczynające się od R to pzysłówki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Sieć słów (Wordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01'), Synset('automobile.v.01')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"automobile\") #jakie są synstety (zbiory synonimow) dla slowa automobile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemma_names() #jakie są słowa w synsecie car.n.01; to synset obejmujący różne synonimy (lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').definition() #jaka jest definicja tego synsetu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he needs a car to get to work']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').examples() #przykladowe uzycie w zdaniu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01'),\n",
       " Synset('car.n.02'),\n",
       " Synset('car.n.03'),\n",
       " Synset('car.n.04'),\n",
       " Synset('cable_car.n.01')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"car\") #jakie są synsety slowa car?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'auto', 'automobile', 'machine', 'motorcar']\n",
      "['car', 'railcar', 'railway_car', 'railroad_car']\n",
      "['car', 'gondola']\n",
      "['car', 'elevator_car']\n",
      "['cable_car', 'car']\n"
     ]
    }
   ],
   "source": [
    "#jak wyglada te 5 synsetow?\n",
    "\n",
    "for synset in wn.synsets(\"car\"):\n",
    "    print(synset.lemma_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Hiponimy i hiperonimy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "motorcar = wn.synset(\"car.n.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('jeep.n.01')\n"
     ]
    }
   ],
   "source": [
    "types_of_cars = motorcar.hyponyms() #hiponim to takie slowo ktorego znaczenie semanatyczne pochodzi od innego slowa (hiperonimu) np rozowy to kolor, gęś to ptak\n",
    "print(types_of_cars[14]) #przykład hiponimu (synsetu zawierającego hiponimy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jeep', 'landrover']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('jeep.n.01').lemma_names() #slowa w synsecie jeep.n.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ambulance', 'beach_wagon', 'station_wagon', 'wagon', 'estate_car', 'beach_waggon', 'station_waggon', 'waggon', 'bus', 'jalopy', 'heap', 'cab', 'hack', 'taxi', 'taxicab', 'compact', 'compact_car', 'convertible', 'coupe', 'cruiser', 'police_cruiser', 'patrol_car', 'police_car', 'prowl_car', 'squad_car', 'electric', 'electric_automobile', 'electric_car', 'gas_guzzler', 'hardtop', 'hatchback', 'horseless_carriage', 'hot_rod', 'hot-rod', 'jeep', 'landrover', 'limousine', 'limo', 'loaner', 'minicar', 'minivan', 'Model_T', 'pace_car', 'racer', 'race_car', 'racing_car', 'roadster', 'runabout', 'two-seater', 'sedan', 'saloon', 'sport_utility', 'sport_utility_vehicle', 'S.U.V.', 'SUV', 'sports_car', 'sport_car', 'Stanley_Steamer', 'stock_car', 'subcompact', 'subcompact_car', 'touring_car', 'phaeton', 'tourer', 'used-car', 'secondhand_car']\n"
     ]
    }
   ],
   "source": [
    "#hiponimy dla slowa car (z synsetu car.n.01)\n",
    "print([lemma.name() for synset in types_of_cars for lemma in synset.lemmas()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('motor_vehicle.n.01')]\n"
     ]
    }
   ],
   "source": [
    "vehicles = motorcar.hypernyms() #hiperonimy (synsety) dla car.n.01\n",
    "print(vehicles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['automotive_vehicle', 'motor_vehicle']\n"
     ]
    }
   ],
   "source": [
    "print(sorted([lemma.name() for synset in vehicles for lemma in synset.lemmas()])) #wszystkie hiperonimy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie8: Znajdź wszystkie hiponimy i hiperonimy dla słowa $dog$. Zacznij od wyszukania synsetów dla tego słowa, wybierz pierwszy z nich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Synonimy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['girl', 'miss', 'missy', 'young_lady', 'young_woman', 'fille', 'female_child', 'girl', 'little_girl', 'daughter', 'girl', 'girlfriend', 'girl', 'lady_friend', 'girl']\n"
     ]
    }
   ],
   "source": [
    "synonyms = []\n",
    "for syn in wn.synsets('girl'):\n",
    "    for lemma in syn.lemmas(): \n",
    "        synonyms.append(lemma.name())\n",
    "\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Antonimy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male_child', 'boy', 'son', 'boy']\n"
     ]
    }
   ],
   "source": [
    "antonyms = []\n",
    "for syn in wn.synsets(\"girl\"):\n",
    "    for l in syn.lemmas():\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie9: Znajdź synonimy i antonimy słowa $happy$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Metryka Wu-Palmera - podobieństwo synsetów\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bicycle.n.01')\n",
      "Synset('male_child.n.01')\n",
      "Synset('homo.n.02')\n"
     ]
    }
   ],
   "source": [
    "bike = wn.synsets('bicycle')[0]\n",
    "boy = wn.synsets('boy')[0]\n",
    "human = wn.synsets('human')[0]\n",
    "\n",
    "print(bike)\n",
    "print(boy)\n",
    "print(human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['homo', 'man', 'human_being', 'human']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('homo.n.02').lemma_names() #zeby srpawdzic co tam jest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.34782608695652173, 0.5217391304347826, 0.4444444444444444)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bike.wup_similarity(human), boy.wup_similarity(human), boy.wup_similarity(bike) #licze na ile slowa (a formalnie synsety są podobne do siebie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie10: Porównaj podobieństwo słów dog, cat i fish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Uzupełnienie: N-gramy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_text = \"This is an example paper in which I summed up some basic concepts reffering to viruses.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> N-gramy dla słów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'is'), ('is', 'an'), ('an', 'example'), ('example', 'paper'), ('paper', 'in'), ('in', 'which'), ('which', 'I'), ('I', 'summed'), ('summed', 'up'), ('up', 'some'), ('some', 'basic'), ('basic', 'concepts'), ('concepts', 'reffering'), ('reffering', 'to'), ('to', 'viruses'), ('viruses', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(word_tokenize(ex_text),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This', 'is', 'an'), ('is', 'an', 'example'), ('an', 'example', 'paper'), ('example', 'paper', 'in'), ('paper', 'in', 'which'), ('in', 'which', 'I'), ('which', 'I', 'summed'), ('I', 'summed', 'up'), ('summed', 'up', 'some'), ('up', 'some', 'basic'), ('some', 'basic', 'concepts'), ('basic', 'concepts', 'reffering'), ('concepts', 'reffering', 'to'), ('reffering', 'to', 'viruses'), ('to', 'viruses', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(word_tokenize(ex_text),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przykład: Zliczanie występowania n-gramów na przykładzie przemówienia Trumpa z 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words =  set(stopwords.words(\"english\"))\n",
    "Trump = [w.lower() for w in inaugural.words('2017-Trump.txt') if w not in stop_words and w not in string.punctuation] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(791, 782)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(ngrams(Trump,3))),len(set(ngrams(Trump,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('we', 'bring', 'back'), 3), (('we', 'make', 'america'), 3), (('united', 'states', 'america'), 2), (('from', 'day', 'forward'), 2), (('together', 'make', 'america'), 2), (('thank', 'god', 'bless'), 2), (('god', 'bless', 'america'), 2), (('chief', 'justice', 'roberts'), 1), (('justice', 'roberts', 'president'), 1), (('roberts', 'president', 'carter'), 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter #Klasa do zliczania\n",
    "\n",
    "counts = Counter(list(ngrams(Trump,3)))\n",
    "print(counts.most_common(10)) #10 najpopularniejszych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> N-gramy dla znaków"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_4grams = []\n",
    "\n",
    "for word in word_tokenize(ex_text):\n",
    "    generated_4grams.append(list(ngrams(word, 4, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('_', '_', '_', 'T'), ('_', '_', 'T', 'h'), ('_', 'T', 'h', 'i'), ('T', 'h', 'i', 's'), ('h', 'i', 's', '_'), ('i', 's', '_', '_'), ('s', '_', '_', '_')], [('_', '_', '_', 'i'), ('_', '_', 'i', 's'), ('_', 'i', 's', '_'), ('i', 's', '_', '_'), ('s', '_', '_', '_')], [('_', '_', '_', 'a'), ('_', '_', 'a', 'n'), ('_', 'a', 'n', '_'), ('a', 'n', '_', '_'), ('n', '_', '_', '_')], [('_', '_', '_', 'e'), ('_', '_', 'e', 'x'), ('_', 'e', 'x', 'a'), ('e', 'x', 'a', 'm'), ('x', 'a', 'm', 'p'), ('a', 'm', 'p', 'l'), ('m', 'p', 'l', 'e'), ('p', 'l', 'e', '_'), ('l', 'e', '_', '_'), ('e', '_', '_', '_')], [('_', '_', '_', 'p'), ('_', '_', 'p', 'a'), ('_', 'p', 'a', 'p'), ('p', 'a', 'p', 'e'), ('a', 'p', 'e', 'r'), ('p', 'e', 'r', '_'), ('e', 'r', '_', '_'), ('r', '_', '_', '_')]]\n"
     ]
    }
   ],
   "source": [
    "print(generated_4grams[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('_', '_', '_', 'T'), ('_', '_', 'T', 'h'), ('_', 'T', 'h', 'i'), ('T', 'h', 'i', 's'), ('h', 'i', 's', '_'), ('i', 's', '_', '_'), ('s', '_', '_', '_'), ('_', '_', '_', 'i'), ('_', '_', 'i', 's'), ('_', 'i', 's', '_')]\n"
     ]
    }
   ],
   "source": [
    "generated_4grams = [word for sublist in generated_4grams for word in sublist] #żeby nie bylo listy list jak wyzej\n",
    "\n",
    "print(generated_4grams[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['___T', '__Th', '_Thi', 'This', 'his_', 'is__', 's___', '___i', '__is', '_is_', 'is__', 's___', '___a', '__an', '_an_', 'an__', 'n___', '___e', '__ex', '_exa', 'exam', 'xamp', 'ampl', 'mple', 'ple_', 'le__', 'e___', '___p', '__pa', '_pap', 'pape', 'aper', 'per_', 'er__', 'r___', '___i', '__in', '_in_', 'in__', 'n___', '___w', '__wh', '_whi', 'whic', 'hich', 'ich_', 'ch__', 'h___', '___I', '__I_', '_I__', 'I___', '___s', '__su', '_sum', 'summ', 'umme', 'mmed', 'med_', 'ed__', 'd___', '___u', '__up', '_up_', 'up__', 'p___', '___s', '__so', '_som', 'some', 'ome_', 'me__', 'e___', '___b', '__ba', '_bas', 'basi', 'asic', 'sic_', 'ic__', 'c___', '___c', '__co', '_con', 'conc', 'once', 'ncep', 'cept', 'epts', 'pts_', 'ts__', 's___', '___r', '__re', '_ref', 'reff', 'effe', 'ffer', 'feri', 'erin', 'ring', 'ing_', 'ng__', 'g___', '___t', '__to', '_to_', 'to__', 'o___', '___v', '__vi', '_vir', 'viru', 'irus', 'ruse', 'uses', 'ses_', 'es__', 's___', '___.', '__._', '_.__', '.___']\n"
     ]
    }
   ],
   "source": [
    "ng_list_4grams = generated_4grams\n",
    "for idx, val in enumerate(generated_4grams):\n",
    "    ng_list_4grams[idx] = ''.join(val)\n",
    "    \n",
    "print(ng_list_4grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Zadanie11: Napisz funkcję która dla zadanego tekstu zwraca słownik zliczeń N-gramów dla znaków (czyli funkcja ma dwa arugmenty: text i N). Przykładowe wywołanie dla N=4 i powyższego tekstu powinno zwrócić:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'___T': 1, '__Th': 1, '_Thi': 1, 'This': 1, 'his_': 1, 'is__': 2, 's___': 4, '___i': 2, '__is': 1, '_is_': 1, '___a': 1, '__an': 1, '_an_': 1, 'an__': 1, 'n___': 2, '___e': 1, '__ex': 1, '_exa': 1, 'exam': 1, 'xamp': 1, 'ampl': 1, 'mple': 1, 'ple_': 1, 'le__': 1, 'e___': 2, '___p': 1, '__pa': 1, '_pap': 1, 'pape': 1, 'aper': 1, 'per_': 1, 'er__': 1, 'r___': 1, '__in': 1, '_in_': 1, 'in__': 1, '___w': 1, '__wh': 1, '_whi': 1, 'whic': 1, 'hich': 1, 'ich_': 1, 'ch__': 1, 'h___': 1, '___I': 1, '__I_': 1, '_I__': 1, 'I___': 1, '___s': 2, '__su': 1, '_sum': 1, 'summ': 1, 'umme': 1, 'mmed': 1, 'med_': 1, 'ed__': 1, 'd___': 1, '___u': 1, '__up': 1, '_up_': 1, 'up__': 1, 'p___': 1, '__so': 1, '_som': 1, 'some': 1, 'ome_': 1, 'me__': 1, '___b': 1, '__ba': 1, '_bas': 1, 'basi': 1, 'asic': 1, 'sic_': 1, 'ic__': 1, 'c___': 1, '___c': 1, '__co': 1, '_con': 1, 'conc': 1, 'once': 1, 'ncep': 1, 'cept': 1, 'epts': 1, 'pts_': 1, 'ts__': 1, '___r': 1, '__re': 1, '_ref': 1, 'reff': 1, 'effe': 1, 'ffer': 1, 'feri': 1, 'erin': 1, 'ring': 1, 'ing_': 1, 'ng__': 1, 'g___': 1, '___t': 1, '__to': 1, '_to_': 1, 'to__': 1, 'o___': 1, '___v': 1, '__vi': 1, '_vir': 1, 'viru': 1, 'irus': 1, 'ruse': 1, 'uses': 1, 'ses_': 1, 'es__': 1, '___.': 1, '__._': 1, '_.__': 1, '.___': 1}\n"
     ]
    }
   ],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Uzupełnienie: Gramatyki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://coling.epfl.ch/TP/corr/TP-parsing-sol.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'b'], ['a'], ['b', 'a']]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.grammar import CFG\n",
    "from nltk.parse.generate import generate\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> A W | B S\n",
    "W -> A S | B W |\n",
    "A -> 'a'\n",
    "B -> 'b'\n",
    "\"\"\")\n",
    "\n",
    "list(generate(grammar, depth = 4)) #depth = maksymalna glębokosc drzewa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'a', 'a'],\n",
       " ['a', 'b', 'b'],\n",
       " ['a', 'b'],\n",
       " ['a'],\n",
       " ['b', 'a', 'b'],\n",
       " ['b', 'a'],\n",
       " ['b', 'b', 'a']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(generate(grammar, depth = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Pytanie: Jak powstało aaa?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
