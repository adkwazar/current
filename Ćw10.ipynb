{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21fcd569",
   "metadata": {},
   "source": [
    "<h3> Generowanie nowego tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b0d9eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c174bf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('cytaty.txt','r')  #otwiram plik z cytatami\n",
    "cytaty = f.read().split('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6c1621f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The best and most beautiful things in the world cannot be seen or even touched - they must be felt with the heart.',\n",
       " 'The best preparation for tomorrow is doing your best today.',\n",
       " 'Put your heart, mind, and soul into even your smallest acts. This is the secret of success.',\n",
       " \"I can't change the direction of the wind, but I can adjust my sails to always reach my destination.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cytaty[:4] #pierwsze cztery cytaty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8422fcd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cytaty) #ile wszysktich cytatow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed536cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From a small seed a mighty trunk may grow.\n"
     ]
    }
   ],
   "source": [
    "cytat9 = cytaty[9] #wybieram przykladowy cytat\n",
    "print(cytat9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "889766eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cytat9) #ile ma znakow?                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53e63533",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set(cytat9)) + [\"<BOS>\", \"<EOS>\"] #tworze liste znakow ktore wystepują w wybranym cytacie, dodaje dwa nowe BOS = początek sekwencji, EOS = koniec sekwencji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b328ea03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', ' ', 'h', 'e', 'k', 't', 'm', 'a', 's', 'w', 'o', 'g', 'F', 'y', 'u', 'r', 'l', 'i', 'n', 'd', '<BOS>', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41a5ab78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab) #ile jest znakow unikalnych?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "660cff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab) #zapisuje te informacje pod zmienną"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3879b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tworze dwa pomocnicze słowniki\n",
    "vocab1 = {s: i for i, s in enumerate(vocab)}\n",
    "vocab2 = {i: s for i, s in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00f1df8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'.': 0, ' ': 1, 'h': 2, 'e': 3, 'k': 4, 't': 5, 'm': 6, 'a': 7, 's': 8, 'w': 9, 'o': 10, 'g': 11, 'F': 12, 'y': 13, 'u': 14, 'r': 15, 'l': 16, 'i': 17, 'n': 18, 'd': 19, '<BOS>': 20, '<EOS>': 21}\n"
     ]
    }
   ],
   "source": [
    "print(vocab1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24b46c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '.', 1: ' ', 2: 'h', 3: 'e', 4: 'k', 5: 't', 6: 'm', 7: 'a', 8: 's', 9: 'w', 10: 'o', 11: 'g', 12: 'F', 13: 'y', 14: 'u', 15: 'r', 16: 'l', 17: 'i', 18: 'n', 19: 'd', 20: '<BOS>', 21: '<EOS>'}\n"
     ]
    }
   ],
   "source": [
    "print(vocab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1936df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Uwaga: tak sie definiuje macierz jednostkową, my będziemy interpretowac wiersze jako reprezentacje one-hot vectors dla znakow\n",
    "torch.eye(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dd8f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: \n",
    "\n",
    "class TextGenerator(nn.Module):\n",
    "    def __init__(self, v_size, hidden_size, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.ident = torch.eye(v_size) #macierz wektorow one-hot encoding dla wszytkich znakow\n",
    "        self.gru = nn.GRU(v_size, hidden_size, n_layers, batch_first=True) #uzyjemy sobie GRU\n",
    "        self.decoder = nn.Linear(hidden_size, v_size) #jako dekoder wezmiemy przeksztalcenie liniowe\n",
    "    \n",
    "    def forward(self, inp, hidden=None):\n",
    "        inp = self.ident[inp]                  #one-hot vector dla kolejnego znaku\n",
    "        output, hidden = self.gru(inp, hidden) #zastosowanie GRU\n",
    "        output = self.decoder(output)          #rozklad kolejnych znakow\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13fee73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextGenerator(vocab_size, 16) #buduje model, z wymiarem dla stanu ukrytego = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d8203c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() #funkcja kosztu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1547e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[20]])\n"
     ]
    }
   ],
   "source": [
    "bos = torch.Tensor([vocab1[\"<BOS>\"]]).long().unsqueeze(0) #zaczynamy od BOS - patrzymy jaki ma indeks\n",
    "print(bos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74840f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1323, -0.3098,  0.1950, -0.1978, -0.1072,  0.1844,  0.0178,\n",
      "          -0.0833, -0.1419,  0.0702, -0.2446,  0.0489,  0.0942,  0.1787,\n",
      "           0.0930, -0.0831,  0.2440,  0.0344,  0.1835, -0.1433, -0.2025,\n",
      "          -0.2081]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output, hidden = model(bos, hidden=None)\n",
    "print(output) #rozklad po pierwszym tokenie - tensor 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1075c046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1323, -0.3098,  0.1950, -0.1978, -0.1072,  0.1844,  0.0178, -0.0833,\n",
      "         -0.1419,  0.0702, -0.2446,  0.0489,  0.0942,  0.1787,  0.0930, -0.0831,\n",
      "          0.2440,  0.0344,  0.1835, -0.1433, -0.2025, -0.2081]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output.reshape(-1, vocab_size)) #tensor 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc29e33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12]])\n"
     ]
    }
   ],
   "source": [
    "target = torch.Tensor([vocab1[cytat9[0]]]).long().unsqueeze(0) #pierwszy znak z cytatu \n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c817b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.9867, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(criterion(output.reshape(-1, vocab_size), target.reshape(-1))) #wartosc funkcji kosztu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3ae397c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c477acc",
   "metadata": {},
   "source": [
    "<h4> Pytanie: Wyjaśnij skąd wzięła się wartość funkcji kosztu (wyznacz ją poprzez bezpośrednie obliczenia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21cae857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0739, -0.2617,  0.1898, -0.2159, -0.1395,  0.1861,  0.0428,\n",
      "          -0.1929, -0.1365,  0.0246, -0.1858, -0.0031,  0.1063,  0.1157,\n",
      "           0.0582, -0.0499,  0.2194,  0.0877,  0.1646, -0.0972, -0.1436,\n",
      "          -0.2369]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output, hidden = model(target, hidden) #uzyj hidden do wygenerowania nowego output i hidden\n",
    "print(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "029d0640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1274, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "target = torch.Tensor([vocab1[cytat9[1]]]).long().unsqueeze(0) #drugi znak w cytacie\n",
    "print(criterion(output.reshape(-1, vocab_size), target.reshape(-1)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c35bfd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor(3.2792, grad_fn=<NllLossBackward0>)\n",
      "3 tensor(2.9959, grad_fn=<NllLossBackward0>)\n",
      "4 tensor(3.2880, grad_fn=<NllLossBackward0>)\n",
      "5 tensor(3.1623, grad_fn=<NllLossBackward0>)\n",
      "6 tensor(3.2368, grad_fn=<NllLossBackward0>)\n",
      "7 tensor(3.2229, grad_fn=<NllLossBackward0>)\n",
      "8 tensor(2.9556, grad_fn=<NllLossBackward0>)\n",
      "9 tensor(3.1574, grad_fn=<NllLossBackward0>)\n",
      "10 tensor(2.9870, grad_fn=<NllLossBackward0>)\n",
      "11 tensor(2.9401, grad_fn=<NllLossBackward0>)\n",
      "12 tensor(3.2907, grad_fn=<NllLossBackward0>)\n",
      "13 tensor(3.2333, grad_fn=<NllLossBackward0>)\n",
      "14 tensor(3.3276, grad_fn=<NllLossBackward0>)\n",
      "15 tensor(3.3419, grad_fn=<NllLossBackward0>)\n",
      "16 tensor(3.2365, grad_fn=<NllLossBackward0>)\n",
      "17 tensor(3.2199, grad_fn=<NllLossBackward0>)\n",
      "18 tensor(3.1820, grad_fn=<NllLossBackward0>)\n",
      "19 tensor(3.2226, grad_fn=<NllLossBackward0>)\n",
      "20 tensor(2.9694, grad_fn=<NllLossBackward0>)\n",
      "21 tensor(3.0715, grad_fn=<NllLossBackward0>)\n",
      "22 tensor(2.9731, grad_fn=<NllLossBackward0>)\n",
      "23 tensor(2.8883, grad_fn=<NllLossBackward0>)\n",
      "24 tensor(2.9984, grad_fn=<NllLossBackward0>)\n",
      "25 tensor(2.9269, grad_fn=<NllLossBackward0>)\n",
      "26 tensor(3.2577, grad_fn=<NllLossBackward0>)\n",
      "27 tensor(3.0141, grad_fn=<NllLossBackward0>)\n",
      "28 tensor(3.1784, grad_fn=<NllLossBackward0>)\n",
      "29 tensor(3.0711, grad_fn=<NllLossBackward0>)\n",
      "30 tensor(2.9292, grad_fn=<NllLossBackward0>)\n",
      "31 tensor(3.1855, grad_fn=<NllLossBackward0>)\n",
      "32 tensor(3.2621, grad_fn=<NllLossBackward0>)\n",
      "33 tensor(3.0282, grad_fn=<NllLossBackward0>)\n",
      "34 tensor(3.1940, grad_fn=<NllLossBackward0>)\n",
      "35 tensor(2.9510, grad_fn=<NllLossBackward0>)\n",
      "36 tensor(3.2423, grad_fn=<NllLossBackward0>)\n",
      "37 tensor(2.9754, grad_fn=<NllLossBackward0>)\n",
      "38 tensor(3.1737, grad_fn=<NllLossBackward0>)\n",
      "39 tensor(3.2484, grad_fn=<NllLossBackward0>)\n",
      "40 tensor(2.9805, grad_fn=<NllLossBackward0>)\n",
      "41 tensor(3.2348, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#analogicznie dla pozotalych znakow\n",
    "for i in range(2, len(cytat9)):\n",
    "    output, hidden = model(target, hidden) #generuje hidden i output w oparciu o poprzedni znak i hidden\n",
    "    target = torch.Tensor([vocab1[cytat9[i]]]).long().unsqueeze(0) #jaki mial byc kolejny znak?\n",
    "    loss = criterion(output.reshape(-1, vocab_size),target.reshape(-1)) #na ile sie pokrywa przewidywany znak z tym co generuje model      \n",
    "    print(i, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5929e5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.3879, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#na koncu dla znaku konca sekwencji\n",
    "output, hidden = model(target, hidden)\n",
    "target = torch.Tensor([vocab1[\"<EOS>\"]]).long().unsqueeze(0)\n",
    "loss = criterion(output.reshape(-1, vocab_size),target.reshape(-1))             \n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e10aee",
   "metadata": {},
   "source": [
    "<h4>Krótsze rozwiązanie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c921e011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<BOS>', 'F', 'r', 'o', 'm', ' ', 'a', ' ', 's', 'm', 'a', 'l', 'l', ' ', 's', 'e', 'e', 'd', ' ', 'a', ' ', 'm', 'i', 'g', 'h', 't', 'y', ' ', 't', 'r', 'u', 'n', 'k', ' ', 'm', 'a', 'y', ' ', 'g', 'r', 'o', 'w', '.', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "cytat_ch = [\"<BOS>\"] + list(cytat9) + [\"<EOS>\"]\n",
    "print(cytat_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5459f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 12, 15, 10, 6, 1, 7, 1, 8, 6, 7, 16, 16, 1, 8, 3, 3, 19, 1, 7, 1, 6, 17, 11, 2, 5, 13, 1, 5, 15, 14, 18, 4, 1, 6, 7, 13, 1, 11, 15, 10, 9, 0, 21]\n"
     ]
    }
   ],
   "source": [
    "cytat_indices = [vocab1[ch] for ch in cytat_ch] #indeksy\n",
    "print(cytat_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc9ddb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[20, 12, 15, 10,  6,  1,  7,  1,  8,  6,  7, 16, 16,  1,  8,  3,  3, 19,\n",
      "          1,  7,  1,  6, 17, 11,  2,  5, 13,  1,  5, 15, 14, 18,  4,  1,  6,  7,\n",
      "         13,  1, 11, 15, 10,  9,  0, 21]])\n"
     ]
    }
   ],
   "source": [
    "cytat_tensor = torch.Tensor(cytat_indices).long().unsqueeze(0) #przerabiamy na tensor\n",
    "print(cytat_tensor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "878123ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 44])\n"
     ]
    }
   ],
   "source": [
    "print(cytat_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c24214f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[20, 12, 15, 10,  6,  1,  7,  1,  8,  6,  7, 16, 16,  1,  8,  3,  3, 19,\n",
      "          1,  7,  1,  6, 17, 11,  2,  5, 13,  1,  5, 15, 14, 18,  4,  1,  6,  7,\n",
      "         13,  1, 11, 15, 10,  9,  0]])\n"
     ]
    }
   ],
   "source": [
    "print(cytat_tensor[:,:-1])  #bez EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5bde8204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12, 15, 10,  6,  1,  7,  1,  8,  6,  7, 16, 16,  1,  8,  3,  3, 19,  1,\n",
      "          7,  1,  6, 17, 11,  2,  5, 13,  1,  5, 15, 14, 18,  4,  1,  6,  7, 13,\n",
      "          1, 11, 15, 10,  9,  0, 21]])\n"
     ]
    }
   ],
   "source": [
    "print(cytat_tensor[:,1:])  #bez BOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13992e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1287, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#mozna przejsc wszystkie znaki bez pętli for\n",
    "\n",
    "output, hidden = model(cytat_tensor[:,:-1])\n",
    "target = cytat_tensor[:,1:]                 \n",
    "loss = criterion(output.reshape(-1, vocab_size), target.reshape(-1))\n",
    "print(loss) #taka zbiorcza wartosc funkcji kosztu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7cb0787d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 300] Loss value: 1.002672553062439\n",
      "[Iter 600] Loss value: 0.13562792539596558\n",
      "[Iter 900] Loss value: 0.04503289610147476\n",
      "[Iter 1200] Loss value: 0.02308017574250698\n",
      "[Iter 1500] Loss value: 0.014185244217514992\n",
      "[Iter 1800] Loss value: 0.009599250741302967\n",
      "[Iter 2100] Loss value: 0.006893750745803118\n",
      "[Iter 2400] Loss value: 0.005152388010174036\n",
      "[Iter 2700] Loss value: 0.003961062524467707\n",
      "[Iter 3000] Loss value: 0.0031090243719518185\n"
     ]
    }
   ],
   "source": [
    "#Trening \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) #optymalizator\n",
    "criterion = nn.CrossEntropyLoss()                          #funkcja kosztu\n",
    "for it in range(3000):\n",
    "    optimizer.zero_grad()\n",
    "    output, hidden = model(cytat_tensor[:,:-1])\n",
    "    loss = criterion(output.reshape(-1, vocab_size),target.reshape(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (it+1) % 300 == 0:  #jak sobie radzi nasz model co 100 epoke?\n",
    "        print(f\"[Iter {it+1}] Loss value: {float(loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0179e740",
   "metadata": {},
   "source": [
    "<h4> Generowanie nowego tekstu na podstawie modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ef300b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja do generowania nowej sekwencji\n",
    "#będziemy lekko modyfikowac prawdopodobienstwa wylosowania danego znaku poprzez czynnik temperaturowy, gdy T=1, to nie zmieniamy, gdy T<1 to zwiększamy prp wystapienia najbardziej prawdopodobnych znakow, gdy T>1 to zmniejszamy prp najbardziej prawdopodobnych znakow\n",
    "\n",
    "\n",
    "def sample_sequence(model, max_len=100, temperature=0.8):\n",
    "    generated_sequence = \"\" #tu będzie przechowywana wygenerowana sekwencja\n",
    "    inp = torch.Tensor([vocab1[\"<BOS>\"]]).long() #zaczynamy od BOS\n",
    "    hidden = None\n",
    "    for p in range(max_len):\n",
    "        output, hidden = model(inp.unsqueeze(0), hidden) #co nam przeiwduje model?\n",
    "        output_dist = output.data.view(-1).div(temperature).exp() #uwzględniamy czynnik temperaturowy\n",
    "        top_i = int(torch.multinomial(output_dist, 1)[0]) #indeks przewidywanego znaku\n",
    "        predicted_char = vocab2[top_i] #przewidywany znak\n",
    "        if predicted_char == \"<EOS>\": #jakim przewidywanym znakiem jest EOS to konczymy\n",
    "            break\n",
    "        generated_sequence += predicted_char #dodajemy kolejny znak\n",
    "        inp = torch.Tensor([top_i]).long() #zapisujemy indeks w postaci tensora\n",
    "    return generated_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ffc250c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From a small seed a mighty trunk may grow.\n"
     ]
    }
   ],
   "source": [
    "print(sample_sequence(model, temperature=0.4)) #T=0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d555f439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From a small seed a mighty trunk may grow.\n"
     ]
    }
   ],
   "source": [
    "print(sample_sequence(model, temperature=1.0)) #T=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "044f24d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fro sa small seed a mighty trunk may grow.\n"
     ]
    }
   ],
   "source": [
    "print(sample_sequence(model, temperature=1.5)) #T=1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb420f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frhmhaar.my gsyyatrunk may grow..\n"
     ]
    }
   ],
   "source": [
    "print(sample_sequence(model, temperature=2.0)) #T=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5649515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yua  g.ghFwmrytygdw rthut ay\n"
     ]
    }
   ],
   "source": [
    "print(sample_sequence(model, temperature=5.0))  #T=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc96d4ee",
   "metadata": {},
   "source": [
    "<h4> Analiza wszytkich cytatów\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "49b26b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator\n",
    "\n",
    "text_field = Field(sequential=True, tokenize=lambda x: x, include_lengths=True, batch_first=True, use_vocab=True, init_token=\"<BOS>\", eos_token=\"<EOS>\")    \n",
    "fields = [('text', text_field)]\n",
    "cytaty = TabularDataset(\"cytaty.txt\", \"csv\", fields) #laduje sobie dane jeszcze raz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d447325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cytaty) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6736c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_field.build_vocab(cytaty)\n",
    "\n",
    "vocab1 = text_field.vocab.stoi #nie musimy tego robic ręcznie\n",
    "vocab2 = text_field.vocab.itos #nie musimy tego robic ręcznie\n",
    "vocab_size = len(text_field.vocab.itos)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97e5ad37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([41, 41, 41, 41, 40, 40, 40, 40, 40, 38, 38, 38, 38, 38, 37, 37])\n",
      "tensor([37, 37, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 35, 35, 35, 35])\n",
      "tensor([86, 85, 84, 84, 83, 83, 83, 83, 82, 82, 82, 82, 81, 81, 81, 80])\n",
      "tensor([35, 34, 34, 34, 33, 33, 33, 33, 32, 32, 31, 31, 30, 30, 30, 30])\n",
      "tensor([63, 63, 63, 62, 62, 61, 61, 61, 61, 60, 60, 60, 60, 59, 59, 59])\n",
      "tensor([68, 68, 68, 67, 66, 66, 65, 65, 65, 65, 64, 64, 64, 64, 63, 63])\n",
      "tensor([79, 79, 78, 78, 77, 77, 77, 77, 77, 76, 76, 76, 76, 75, 75, 75])\n",
      "tensor([58, 58, 58, 58, 58, 57, 57, 57, 57, 56, 56, 56, 56, 55, 55, 55])\n",
      "tensor([46, 46, 46, 46, 45, 45, 45, 45, 45, 45, 44, 44, 44, 44, 44, 44])\n",
      "tensor([49, 49, 48, 48, 48, 48, 48, 48, 48, 48, 48, 47, 47, 47, 46, 46])\n",
      "tensor([74, 74, 73, 72, 71, 71, 71, 71, 71, 70, 69, 69, 69, 68, 68, 68])\n",
      "tensor([51, 51, 51, 51, 50, 50, 50, 50, 50, 50, 50, 49, 49, 49, 49, 49])\n",
      "tensor([44, 44, 44, 43, 43, 43, 43, 43, 43, 42, 42, 42, 42, 42, 42, 42])\n",
      "tensor([28, 27, 27, 27, 26, 26, 26, 26, 25, 25, 25, 24, 24, 24, 24, 24])\n",
      "tensor([108, 107, 107, 106, 104, 103, 102, 102, 102, 101, 101, 101,  98,  98,\n",
      "         97,  97])\n",
      "tensor([24, 24, 23, 23, 23, 23, 23, 23, 23, 23, 22, 22, 22, 22, 22, 22])\n",
      "tensor([172, 170, 168, 168, 166, 165, 164, 161, 160, 160, 158, 157, 156, 155,\n",
      "        152, 150])\n",
      "tensor([12, 12, 12, 11, 11, 11,  9,  9,  9,  9,  8,  8,  8,  7,  6,  4])\n",
      "tensor([229, 225, 216, 207, 200, 196, 195, 194, 193, 186, 185, 178, 175, 174,\n",
      "        173, 173])\n",
      "tensor([247, 247, 235])\n",
      "tensor([21, 21, 21, 20, 20, 20, 20, 20, 20, 18, 18, 17, 17, 16, 15, 12])\n",
      "tensor([30, 30, 30, 30, 30, 30, 30, 29, 29, 29, 29, 29, 29, 29, 28, 28])\n",
      "tensor([96, 96, 95, 95, 95, 95, 94, 94, 93, 90, 90, 89, 89, 89, 87, 86])\n",
      "tensor([131, 130, 126, 125, 123, 121, 119, 119, 118, 116, 116, 115, 113, 113,\n",
      "        109, 109])\n",
      "tensor([150, 149, 148, 146, 144, 142, 142, 141, 141, 140, 137, 137, 136, 135,\n",
      "        133, 133])\n",
      "tensor([55, 55, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 53, 53, 52, 52])\n"
     ]
    }
   ],
   "source": [
    "#Batchowanie w sieciach rekurencyjnych - wstawka\n",
    "data_iter = BucketIterator(cytaty, batch_size=16, sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
    "\n",
    "for (cytat, lengths), label in data_iter:\n",
    "    print(lengths) #dlugosc cytatow w jednym batchu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "12140583",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja do trenowania modelu\n",
    "\n",
    "def train(model, data, batch_size=1, num_epochs=1, lr=0.001, print_every=100):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) #optymalizator - Adam\n",
    "    criterion = nn.CrossEntropyLoss()  #Entropia krzyżowa jako funkcja koztu\n",
    "    it = 0  #licnzik iteraji\n",
    "    data_iter = BucketIterator(data, batch_size=batch_size, sort_key=lambda x: len(x.text), sort_within_batch=True)\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        avg_loss = 0 #srednia wartosc funkcji kosztu\n",
    "        for (tweet, lengths), label in data_iter:\n",
    "            target = tweet[:, 1:] #bez BOS\n",
    "            inp = tweet[:, :-1]  #bez EOS\n",
    "            optimizer.zero_grad()\n",
    "            output, hidden = model(inp)\n",
    "            loss = criterion(output.reshape(-1, vocab_size), target.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss\n",
    "            it += 1 # inkrementujemy liczbe iteracji\n",
    "            \n",
    "            if it % print_every == 0:  #co setną ierację pokaz srednią wartosc funkcji kosztu\n",
    "                print(f\"[Iter {it+1}] Loss {float(avg_loss/print_every)}\")\n",
    "                avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "600ca8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextGenerator(vocab_size, 40) #budujemy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "97281f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 101] Loss 0.22996553778648376\n",
      "[Iter 201] Loss 0.1098194569349289\n",
      "[Iter 301] Loss 0.015256743878126144\n",
      "[Iter 401] Loss 0.1886453777551651\n",
      "[Iter 501] Loss 0.10235956311225891\n",
      "[Iter 601] Loss 0.03265569359064102\n",
      "[Iter 701] Loss 0.17500537633895874\n",
      "[Iter 801] Loss 0.10474497079849243\n",
      "[Iter 901] Loss 0.043717317283153534\n",
      "[Iter 1001] Loss 0.17315396666526794\n",
      "[Iter 1101] Loss 0.11181270331144333\n",
      "[Iter 1201] Loss 0.05601189658045769\n",
      "[Iter 1301] Loss 0.17299970984458923\n",
      "[Iter 1401] Loss 0.1137675940990448\n",
      "[Iter 1501] Loss 0.06057059392333031\n",
      "[Iter 1601] Loss 0.007530189584940672\n",
      "[Iter 1701] Loss 0.13070720434188843\n",
      "[Iter 1801] Loss 0.0664728432893753\n",
      "[Iter 1901] Loss 0.026708440855145454\n",
      "[Iter 2001] Loss 0.1339685022830963\n",
      "[Iter 2101] Loss 0.08049748092889786\n",
      "[Iter 2201] Loss 0.03516561910510063\n",
      "[Iter 2301] Loss 0.1420387625694275\n",
      "[Iter 2401] Loss 0.08406105637550354\n",
      "[Iter 2501] Loss 0.038467887789011\n",
      "[Iter 2601] Loss 0.14875061810016632\n"
     ]
    }
   ],
   "source": [
    "train(model, cytaty, batch_size=32, num_epochs=200, lr=0.004, print_every=100) #trenujemy model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f2a1bc",
   "metadata": {},
   "source": [
    "<h4> Pytanie: Dlaczego liczba iteracji wynosiła ok 2600 skoro zadaliśmy liczbę epok na 200?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f10fe",
   "metadata": {},
   "source": [
    "<h4> Pytanie: Poniżej wygenerowano dwie sekwencje (losowe cytaty) - jeden z temperaturą 0.6, drugi z 1.5. Który pochodzi z którego losowania? Dlaczego?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4978e7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe to be a drouds the stiveled a alless inspirational of nows when I drife we like to mig to \n"
     ]
    }
   ],
   "source": [
    "print(sample_sequence(model, temperature = T))  #T=?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "95699343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Withit maken van sigrent.\n"
     ]
    }
   ],
   "source": [
    "print(sample_sequence(model, temperature = T))  #T=?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
